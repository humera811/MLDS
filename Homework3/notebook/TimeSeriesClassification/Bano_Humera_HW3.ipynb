{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Name - Humera Bano\n",
    "2. Github Username - humera811\n",
    "3. USC ID - 7255691039"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organizing all imports together\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import warnings\n",
    "import csv as csv\n",
    "from os.path import join, getsize\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Time Series Classification__\n",
    "Part 1: Feature Creation/Extraction\n",
    "An interesting task in machine learning is classification of time series. In this problem,\n",
    "we will classify the activities of humans based on time series obtained by a Wireless\n",
    "Sensor Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __(a) Download the AReM data from: https://archive.ics.uci.edu/ml/datasets/ Activity+Recognition+system+based+on+Multisensor+data+fusion+\\%28AReM\\ %29 . The dataset contains 7 folders that represent seven types of activities. In each folder, there are multiple files each of which represents an instant of a human performing an activity.1 Each file containis 6 time series collected from activities of the same person, which are called avg rss12, var rss12, avg rss13, var rss13, vg rss23, and ar rss23. There are 88 instances in the dataset, each of which contains 6 time series and each time series has 480 consecutive values.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing extra comma in cycling dataset9.csv and cycling dataset14.csv manually as suggested on piazza\n",
    "# bending2 dataset4.csv space separated (delimitter/sep_)\n",
    "# https://www.geeksforgeeks.org/how-to-use-glob-function-to-find-files-recursively-in-python/\n",
    "path = \"../../data/AReM/\"\n",
    "actv_dir = ['bending1', 'bending2']\n",
    "actv_dir2 = ['bending1','bending2','cycling', 'lying', 'sitting', 'standing', 'walking']\n",
    "features = ['mean_', 'std_', 'min_', 'first_quartile_', 'median_', 'third_quartile_','max_']\n",
    "columns = ['# Columns: time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __(b) Keep datasets 1 and 2 in folders bending1 and bending 2, as well as datasets 1, 2, and 3 in other folders as test data and other datasets as train data.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/22809061/read-space-separated-data-with-pandas\n",
    "# https://www.summet.com/dmsi/html/CSVdataFiles.html\n",
    "# https://pandas.pydata.org/pandas-docs/version/0.25.0/reference/api/pandas.DataFrame.T.html\n",
    "# https://docs.python.org/3/library/os.html\n",
    "# https://www.tutorialspoint.com/python/os_walk.htm\n",
    "# https://www.geeksforgeeks.org/python-os-path-splitext-method/\n",
    "\n",
    "# !/usr/bin/python\n",
    "def train_or_test(root):\n",
    "    trainset = []\n",
    "    testset = []\n",
    "    # scanning bottom-to-up\n",
    "    for root, dirs, files in os.walk(path, topdown=False):\n",
    "        for name in files:\n",
    "            # Split the path in root and ext pair\n",
    "            root_ext = os.path.splitext(name)\n",
    "            root_val = root_ext[1]\n",
    "            if (root_val.endswith('.csv')):\n",
    "                # https://www.geeksforgeeks.org/python-regex-re-search-vs-re-findall/\n",
    "                file_path = os.path.join(root, name)\n",
    "                # print(file_path)\n",
    "                my_path = os.path.split(root)\n",
    "                feature_act = my_path[-1]\n",
    "                # my_path = Path(path)\n",
    "                # activity = my_path.stem\n",
    "                regex = '\\d+'\n",
    "                match = re.findall(regex, name)\n",
    "                # for bending 1 and 2\n",
    "                match_activity = int(match[0])\n",
    "                if (match_activity <= 2 or (feature_act not in actv_dir and match_activity == 3)):\n",
    "                    testset.append(file_path)\n",
    "                else:\n",
    "                    trainset.append(file_path)\n",
    "    return trainset, testset\n",
    "train_set, test_set = train_or_test(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __(c) Feature Extraction Classification of time series usually needs extracting features from them.In this problem, we focus on time-domain features__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __i. Research what types of time-domain features are usually used in time series classification and list them (examples are minimum, maximum, mean, etc)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time-domain features are usually used in time series classification are:\n",
    "1. Means\n",
    "2. Standard Deviations\n",
    "3. Skewness, Kurtosis\n",
    "4. Maximum and minimum values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __ii. Extract the time-domain features minimum, maximum, mean, median, standard deviation, first quartile, and third quartile for all of the 6 time series in each instance.You are free to normalize/standardize features or use them directly__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/18366797/pandas-read-csv-how-to-skip-comment-lines\n",
    "# https://stackoverflow.com/questions/25440008/python-pandas-flatten-a-dataframe-to-a-list\n",
    "csv_files = glob.glob('../../data/AReM/*/*.csv', recursive=True)\n",
    "# print(csv_files)\n",
    "sniffer = csv.Sniffer()\n",
    "\n",
    "def load_data(csv_files, activity, columns, split=1, standardize=False):\n",
    "    column_names = [desc + str(i) for i in range(1, 7) for desc in features]\n",
    "    train_test_instances  = [] \n",
    "    train_test_labels = []\n",
    "    train_instances = []\n",
    "    test_instances = []\n",
    "    train_label=[]\n",
    "    test_label=[]\n",
    "    for csv in csv_files:\n",
    "        root = os.path.dirname(csv)\n",
    "        my_path = os.path.split(root)\n",
    "        feature_act = my_path[-1]\n",
    "        # print(feature_act)\n",
    "        train_test_labels.append(feature_act)\n",
    "        #separator\n",
    "        # https://stackoverflow.com/questions/45732459/retrieve-delimiter-infered-by-read-csv-in-pandas\n",
    "        sep_ = sniffer.sniff(open(csv).read())\n",
    "        sep = sep_.delimiter\n",
    "        if sep == ' ':\n",
    "            df = pd.read_csv(csv, comment = '#', header=None, sep=\"\\s+\")\n",
    "        else:\n",
    "            df = pd.read_csv(csv, comment = '#', header=None)\n",
    "            \n",
    "        df.columns = ['# Columns: time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
    "        # finding length of each split\n",
    "        split_length = int(df.shape[0] / split)\n",
    "        train_test_seg = []\n",
    "        for j in range(split):\n",
    "            df_describe = df[j*split_length : min((j+1)*split_length,df.shape[0])]\n",
    "            df_describe = df_describe.describe().drop('count').drop(columns=\"# Columns: time\").T\n",
    "            train_test_seg.append(df_describe.to_numpy().flatten())\n",
    "        train_test_instances.append(np.concatenate(train_test_seg, axis=0))      \n",
    "    df_new = pd.DataFrame(train_test_instances, columns=column_names)\n",
    "    df_new = df_new.loc[:, activity]\n",
    "    df_new['label'] = pd.Series(train_test_labels) # adding label as suggested on piazza\n",
    "    return df_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_1</th>\n",
       "      <th>max_1</th>\n",
       "      <th>mean_1</th>\n",
       "      <th>median_1</th>\n",
       "      <th>std_1</th>\n",
       "      <th>first_quartile_1</th>\n",
       "      <th>third_quartile_1</th>\n",
       "      <th>min_2</th>\n",
       "      <th>max_2</th>\n",
       "      <th>mean_2</th>\n",
       "      <th>...</th>\n",
       "      <th>first_quartile_5</th>\n",
       "      <th>third_quartile_5</th>\n",
       "      <th>min_6</th>\n",
       "      <th>max_6</th>\n",
       "      <th>mean_6</th>\n",
       "      <th>median_6</th>\n",
       "      <th>std_6</th>\n",
       "      <th>first_quartile_6</th>\n",
       "      <th>third_quartile_6</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36.25</td>\n",
       "      <td>48.00</td>\n",
       "      <td>43.969125</td>\n",
       "      <td>44.50</td>\n",
       "      <td>1.618364</td>\n",
       "      <td>43.3100</td>\n",
       "      <td>44.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.413125</td>\n",
       "      <td>...</td>\n",
       "      <td>20.5000</td>\n",
       "      <td>23.7500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0.555312</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.487826</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8300</td>\n",
       "      <td>bending1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.00</td>\n",
       "      <td>48.00</td>\n",
       "      <td>43.454958</td>\n",
       "      <td>43.25</td>\n",
       "      <td>1.386098</td>\n",
       "      <td>42.5000</td>\n",
       "      <td>45.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.378083</td>\n",
       "      <td>...</td>\n",
       "      <td>22.2500</td>\n",
       "      <td>24.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.26</td>\n",
       "      <td>0.679646</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.622534</td>\n",
       "      <td>0.4300</td>\n",
       "      <td>0.8700</td>\n",
       "      <td>bending1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.00</td>\n",
       "      <td>47.75</td>\n",
       "      <td>42.179812</td>\n",
       "      <td>43.50</td>\n",
       "      <td>3.670666</td>\n",
       "      <td>39.1500</td>\n",
       "      <td>45.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.696042</td>\n",
       "      <td>...</td>\n",
       "      <td>30.4575</td>\n",
       "      <td>36.3300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.613521</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.524317</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>bending1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.00</td>\n",
       "      <td>45.75</td>\n",
       "      <td>41.678063</td>\n",
       "      <td>41.75</td>\n",
       "      <td>2.243490</td>\n",
       "      <td>41.3300</td>\n",
       "      <td>42.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.83</td>\n",
       "      <td>0.535979</td>\n",
       "      <td>...</td>\n",
       "      <td>28.4575</td>\n",
       "      <td>31.2500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.383292</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.389164</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>bending1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.00</td>\n",
       "      <td>47.40</td>\n",
       "      <td>43.954500</td>\n",
       "      <td>44.33</td>\n",
       "      <td>1.558835</td>\n",
       "      <td>43.0000</td>\n",
       "      <td>45.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.426250</td>\n",
       "      <td>...</td>\n",
       "      <td>35.3625</td>\n",
       "      <td>36.5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.493292</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.513506</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9400</td>\n",
       "      <td>bending1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>18.50</td>\n",
       "      <td>44.25</td>\n",
       "      <td>35.752354</td>\n",
       "      <td>36.00</td>\n",
       "      <td>4.614802</td>\n",
       "      <td>33.0000</td>\n",
       "      <td>39.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.60</td>\n",
       "      <td>3.328104</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0000</td>\n",
       "      <td>18.0625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.39</td>\n",
       "      <td>3.069667</td>\n",
       "      <td>2.770</td>\n",
       "      <td>1.748326</td>\n",
       "      <td>1.7975</td>\n",
       "      <td>4.0600</td>\n",
       "      <td>cycling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>24.25</td>\n",
       "      <td>45.00</td>\n",
       "      <td>37.177042</td>\n",
       "      <td>36.25</td>\n",
       "      <td>3.581301</td>\n",
       "      <td>34.5000</td>\n",
       "      <td>40.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.58</td>\n",
       "      <td>2.374208</td>\n",
       "      <td>...</td>\n",
       "      <td>17.9500</td>\n",
       "      <td>21.7500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.34</td>\n",
       "      <td>2.921729</td>\n",
       "      <td>2.500</td>\n",
       "      <td>1.852600</td>\n",
       "      <td>1.5000</td>\n",
       "      <td>3.9000</td>\n",
       "      <td>cycling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>23.33</td>\n",
       "      <td>43.50</td>\n",
       "      <td>36.244083</td>\n",
       "      <td>36.75</td>\n",
       "      <td>3.822016</td>\n",
       "      <td>33.4575</td>\n",
       "      <td>39.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.71</td>\n",
       "      <td>2.736021</td>\n",
       "      <td>...</td>\n",
       "      <td>15.7500</td>\n",
       "      <td>21.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.15</td>\n",
       "      <td>3.530500</td>\n",
       "      <td>3.110</td>\n",
       "      <td>1.963685</td>\n",
       "      <td>2.1700</td>\n",
       "      <td>4.6175</td>\n",
       "      <td>cycling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>26.25</td>\n",
       "      <td>44.25</td>\n",
       "      <td>36.957458</td>\n",
       "      <td>36.29</td>\n",
       "      <td>3.434863</td>\n",
       "      <td>34.5000</td>\n",
       "      <td>40.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.64</td>\n",
       "      <td>2.420083</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0000</td>\n",
       "      <td>18.2500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.34</td>\n",
       "      <td>2.934625</td>\n",
       "      <td>2.525</td>\n",
       "      <td>1.631380</td>\n",
       "      <td>1.6600</td>\n",
       "      <td>4.0300</td>\n",
       "      <td>cycling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>27.75</td>\n",
       "      <td>44.67</td>\n",
       "      <td>37.144833</td>\n",
       "      <td>36.33</td>\n",
       "      <td>3.758904</td>\n",
       "      <td>34.0000</td>\n",
       "      <td>40.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.76</td>\n",
       "      <td>2.419062</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>18.7500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.75</td>\n",
       "      <td>2.822437</td>\n",
       "      <td>2.590</td>\n",
       "      <td>1.637183</td>\n",
       "      <td>1.5800</td>\n",
       "      <td>3.7400</td>\n",
       "      <td>cycling</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    min_1  max_1     mean_1  median_1     std_1  first_quartile_1  \\\n",
       "0   36.25  48.00  43.969125     44.50  1.618364           43.3100   \n",
       "1   37.00  48.00  43.454958     43.25  1.386098           42.5000   \n",
       "2   33.00  47.75  42.179812     43.50  3.670666           39.1500   \n",
       "3   33.00  45.75  41.678063     41.75  2.243490           41.3300   \n",
       "4   35.00  47.40  43.954500     44.33  1.558835           43.0000   \n",
       "..    ...    ...        ...       ...       ...               ...   \n",
       "64  18.50  44.25  35.752354     36.00  4.614802           33.0000   \n",
       "65  24.25  45.00  37.177042     36.25  3.581301           34.5000   \n",
       "66  23.33  43.50  36.244083     36.75  3.822016           33.4575   \n",
       "67  26.25  44.25  36.957458     36.29  3.434863           34.5000   \n",
       "68  27.75  44.67  37.144833     36.33  3.758904           34.0000   \n",
       "\n",
       "    third_quartile_1  min_2  max_2    mean_2  ...  first_quartile_5  \\\n",
       "0              44.67    0.0   1.50  0.413125  ...           20.5000   \n",
       "1              45.00    0.0   1.58  0.378083  ...           22.2500   \n",
       "2              45.00    0.0   3.00  0.696042  ...           30.4575   \n",
       "3              42.75    0.0   2.83  0.535979  ...           28.4575   \n",
       "4              45.00    0.0   1.70  0.426250  ...           35.3625   \n",
       "..               ...    ...    ...       ...  ...               ...   \n",
       "64             39.33    0.0  12.60  3.328104  ...           14.0000   \n",
       "65             40.25    0.0   8.58  2.374208  ...           17.9500   \n",
       "66             39.25    0.0   9.71  2.736021  ...           15.7500   \n",
       "67             40.25    0.0   8.64  2.420083  ...           14.0000   \n",
       "68             40.50    0.0  10.76  2.419062  ...           15.0000   \n",
       "\n",
       "    third_quartile_5  min_6  max_6    mean_6  median_6     std_6  \\\n",
       "0            23.7500    0.0   2.96  0.555312     0.490  0.487826   \n",
       "1            24.0000    0.0   5.26  0.679646     0.500  0.622534   \n",
       "2            36.3300    0.0   2.18  0.613521     0.500  0.524317   \n",
       "3            31.2500    0.0   1.79  0.383292     0.430  0.389164   \n",
       "4            36.5000    0.0   1.79  0.493292     0.430  0.513506   \n",
       "..               ...    ...    ...       ...       ...       ...   \n",
       "64           18.0625    0.0   9.39  3.069667     2.770  1.748326   \n",
       "65           21.7500    0.0   9.34  2.921729     2.500  1.852600   \n",
       "66           21.0000    0.0  11.15  3.530500     3.110  1.963685   \n",
       "67           18.2500    0.0   8.34  2.934625     2.525  1.631380   \n",
       "68           18.7500    0.0   8.75  2.822437     2.590  1.637183   \n",
       "\n",
       "    first_quartile_6  third_quartile_6     label  \n",
       "0             0.0000            0.8300  bending1  \n",
       "1             0.4300            0.8700  bending1  \n",
       "2             0.0000            1.0000  bending1  \n",
       "3             0.0000            0.5000  bending1  \n",
       "4             0.0000            0.9400  bending1  \n",
       "..               ...               ...       ...  \n",
       "64            1.7975            4.0600   cycling  \n",
       "65            1.5000            3.9000   cycling  \n",
       "66            2.1700            4.6175   cycling  \n",
       "67            1.6600            4.0300   cycling  \n",
       "68            1.5800            3.7400   cycling  \n",
       "\n",
       "[69 rows x 43 columns]"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_columns = ['min_','max_','mean_','median_','std_','first_quartile_', 'third_quartile_']\n",
    "activity = [stat + str(i) for i in range(1, 7) for stat in new_columns]\n",
    "#trainset\n",
    "train_stats = load_data(train_set, activity, columns)\n",
    "# train_data.insert(loc=0, column='row_num', value=np.arange(len(train_stats)))\n",
    "# train_stats.isnull().any()\n",
    "train_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min_1               False\n",
       "max_1               False\n",
       "mean_1              False\n",
       "median_1            False\n",
       "std_1               False\n",
       "first_quartile_1    False\n",
       "third_quartile_1    False\n",
       "min_2               False\n",
       "max_2               False\n",
       "mean_2              False\n",
       "median_2            False\n",
       "std_2               False\n",
       "first_quartile_2    False\n",
       "third_quartile_2    False\n",
       "min_3               False\n",
       "max_3               False\n",
       "mean_3              False\n",
       "median_3            False\n",
       "std_3               False\n",
       "first_quartile_3    False\n",
       "third_quartile_3    False\n",
       "min_4               False\n",
       "max_4               False\n",
       "mean_4              False\n",
       "median_4            False\n",
       "std_4               False\n",
       "first_quartile_4    False\n",
       "third_quartile_4    False\n",
       "min_5               False\n",
       "max_5               False\n",
       "mean_5              False\n",
       "median_5            False\n",
       "std_5               False\n",
       "first_quartile_5    False\n",
       "third_quartile_5    False\n",
       "min_6               False\n",
       "max_6               False\n",
       "mean_6              False\n",
       "median_6            False\n",
       "std_6               False\n",
       "first_quartile_6    False\n",
       "third_quartile_6    False\n",
       "label               False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stats = load_data(test_set, activity, columns)\n",
    "# test_stats.isnull().any()\n",
    "test_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __1. (c). iii. Estimate the standard deviation of each of the time-domain features you extracted from the data. Then, use Python’s bootstrapped or any other method to build a 90% bootsrap confidence interval for the standard deviation of each feature.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min_1               8.762140\n",
       "max_1               4.412987\n",
       "mean_1              4.899736\n",
       "median_1            4.937990\n",
       "std_1               1.752199\n",
       "first_quartile_1    5.710307\n",
       "third_quartile_1    4.766155\n",
       "min_2               0.000000\n",
       "max_2               5.129019\n",
       "mean_2              1.594808\n",
       "median_2            1.431649\n",
       "std_2               0.899469\n",
       "first_quartile_2    0.948719\n",
       "third_quartile_2    2.150367\n",
       "min_3               3.042703\n",
       "max_3               4.742450\n",
       "mean_3              3.849179\n",
       "median_3            3.831669\n",
       "std_3               0.992367\n",
       "first_quartile_3    4.130098\n",
       "third_quartile_3    3.931595\n",
       "min_4               0.000000\n",
       "max_4               2.293990\n",
       "mean_4              1.175467\n",
       "median_4            1.145382\n",
       "std_4               0.471793\n",
       "first_quartile_4    0.839690\n",
       "third_quartile_4    1.560837\n",
       "min_5               5.349156\n",
       "max_5               5.429800\n",
       "mean_5              5.101791\n",
       "median_5            5.248155\n",
       "std_5               1.054104\n",
       "first_quartile_5    5.523612\n",
       "third_quartile_5    4.939105\n",
       "min_6               0.051577\n",
       "max_6               2.530878\n",
       "mean_6              1.167023\n",
       "median_6            1.100587\n",
       "std_6               0.517478\n",
       "first_quartile_6    0.771402\n",
       "third_quartile_6    1.544550\n",
       "Name: std, dtype: float64"
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set = train_stats.append(train_stats, ignore_index=True)\n",
    "data_set.describe().loc['std']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
